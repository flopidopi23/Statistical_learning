---
title: "Assignment2"
author: "Florencia Luque and Simon Schemtz"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(ellipse)
library(jpeg)
library(mvtnorm)
library(plotly)
library(rgl)
```

# Introduction
This document presents the work done for Assignment 3 in the Statistical Learning course at Universidad Carlos III de Madrid. The objective of this assignment is to implement the Expectation-Maximization (EM) algorithm for estimating the parameters of a Gaussian Mixture Model (GMM) across different data dimensions.

Specifically, the assignment requires:

* Implementing the EM algorithm for Gaussian mixtures in 1D, 2D, and higher dimensions.
* Validating the implementation on synthetic datasets, providing both visual and analytical results.
* Applying the algorithm to image segmentation, demonstrating its ability to classify different regions within an image.

This report details the methodology, results, and insights gained from the implementation and testing of the EM algorithm.
# Functions
## Initialization function 
Initialization is the first step in the EM algorithm and is crucial because convergence depends heavily on its setup, while the quality of the initial clusters influences the final solution. In this implementation, we initialize the cluster means by randomly selecting KK data points, ensuring that the means start within a reasonable range. The covariance matrices are set to identity matrices to maintain numerical stability and prevent singularities. The mixture coefficients are uniformly initialized to ensure that no cluster dominates the process at the start. This method provides a simple and effective way to initialize EM without relying on K-means, making the procedure fully autonomous.
```{r}
initialize_em_parameters = function(X, K) {
  N = nrow(X)  # Data rows
  D = ncol(X)  # Data columns
  
  # Randomly select K data points as initial means
  mu = X[sample(1:N, K), ]
  
  # Initialize covariance matrices as identity matrices
  sigma = array(diag(D), dim = c(D, D, K))
  
  # Initialize equal mixing coefficients
  pi_k = rep(1/K, K)
  
  return(list(mu = mu, sigma = sigma, pi_k = pi_k))
}
```
## Plot 1D and 2D functions

Generate plot for 1D and 2D image
```{r}
library(plotly)
library(rgl)

library(plotly)
library(mvtnorm)
library(ggplot2)
library(MASS)  # For ellipse generation

# Function to compute 2D Gaussian ellipses
get_ellipse <- function(mu, sigma, npoints = 100) {
  angles <- seq(0, 2 * pi, length.out = npoints)
  circle <- cbind(cos(angles), sin(angles))  # Unit circle
  chol_decomp <- chol(sigma)  # Cholesky decomposition of covariance
  ellipse_points <- circle %*% chol_decomp  # Transform to Gaussian shape
  ellipse_points <- sweep(ellipse_points, 2, mu, "+")  # Shift to mean
  return(as.data.frame(ellipse_points))
}

# Function to plot 1D, 2D, and 3D GMM results
plot_1D_2D_3D = function(X, gamma, mu, sigma) {
  D = ncol(X)  # Dimension of data (1D, 2D, or 3D)
  K = nrow(as.matrix(mu))  # Ensure mu is a matrix
  cluster_labels = apply(gamma, 1, which.max)  # Assign each point to a cluster
  
  data_plot = as.data.frame(X)
  data_plot$cluster = as.factor(cluster_labels)  # Convert to categorical
  
  # ---- 1D Case ----
  if (D == 1) {
    p <- ggplot(data_plot, aes(x = V1, fill = cluster)) +
      geom_histogram(alpha = 0.5, bins = 30, position = "identity") +
      geom_vline(xintercept = as.vector(mu), col = "black", linetype = "dashed", size = 1) +
      labs(title = "1D Gaussian Mixture Clustering", x = "Value", y = "Frequency") +
      theme_minimal()
    return(p)
    
  # ---- 2D Case ----
  } else if (D == 2) {
    p <- ggplot(data_plot, aes(x = V1, y = V2, color = cluster)) +
      geom_point(alpha = 0.6) +  # Data points
      geom_point(data = as.data.frame(mu), aes(x = V1, y = V2), 
                 color = "black", size = 4, shape = 4) +  # Cluster means
    
      # ---- Add Gaussian Ellipses ----
      lapply(1:K, function(k) {
        ellipse_data <- get_ellipse(mu[k,], sigma[,,k])
        geom_path(data = ellipse_data, aes(x = V1, y = V2), color = "black", linetype = "dashed")
      }) +
      
      labs(title = "2D Gaussian Mixture Clustering", x = "X", y = "Y") +
      theme_minimal()
    return(p)
    
  # ---- 3D Case ----
  } else if (D == 3) {
    # 3D Plot using plotly (same as before)
    plot_3D <- plot_ly() %>%
      add_trace(
        data = data_plot, 
        x = ~V1, y = ~V2, z = ~V3, 
        type = "scatter3d", mode = "markers",
        marker = list(size = 3, opacity = 0.7),
        color = ~cluster  # Ensure color length matches points
      ) %>%
      add_trace(
        data = as.data.frame(mu), 
        x = ~V1, y = ~V2, z = ~V3, 
        type = "scatter3d", mode = "markers",
        marker = list(size = 6, color = "black", symbol = "x")
      ) %>%
      layout(
        title = "3D Gaussian Mixture Clustering",
        scene = list(xaxis = list(title = "X"), 
                     yaxis = list(title = "Y"), 
                     zaxis = list(title = "Z"))
      )
    
    return(plot_3D)
    
  } else {
    stop("This function only supports 1D, 2D, and 3D data!")
  }
}


# ---- Function to Generate 3D Ellipsoid Points ----
get_ellipsoid <- function(mean_vec, cov_mat, npoints = 20) {
  angles <- expand.grid(seq(0, pi, length.out = npoints), seq(0, 2 * pi, length.out = npoints))
  
  # Eigen decomposition for ellipsoid transformation
  eigen_decomp <- eigen(cov_mat)
  radii <- sqrt(eigen_decomp$values)
  rotation <- eigen_decomp$vectors
  
  # Generate unit sphere points
  x <- radii[1] * sin(angles[,1]) * cos(angles[,2])
  y <- radii[2] * sin(angles[,1]) * sin(angles[,2])
  z <- radii[3] * cos(angles[,1])
  
  sphere_points <- cbind(x, y, z)
  
  # Transform using eigenvectors (rotation) and add mean
  ellipsoid_points <- t(rotation %*% t(sphere_points)) + matrix(rep(mean_vec, npoints^2), ncol = 3, byrow = TRUE)
  return(ellipsoid_points)
}

```

## Plot 3D or more function (for images)

```{r}
library(jpeg)
library(imager)

reconstruct_image <- function(image_path, em_result, output_path = "segmented_image.jpg") {
  # Load the original image
  img <- readJPEG(image_path)
  dim_img <- dim(img)
  
  # Get cluster assignments (each pixel assigned to a cluster)
  clusters <- apply(em_result$gamma, 1, which.max)
  
  # Reconstruct the image using the cluster means
  segmented_matrix <- em_result$mu[clusters, ]
  
  # Reshape back into the original image format
  segmented_img <- array(segmented_matrix, dim = dim_img)
  
  # Save the segmented image
  writeJPEG(segmented_img, output_path)
  
  # Display the segmented image
  plot(as.cimg(segmented_img))
  
  # Return the segmented image matrix
  return(segmented_img)
}



```

## function for log likelihood convergency 

```{r}
plot_log_likelihood = function(log_likelihood_values) {
  iterations = 1:length(log_likelihood_values)
  log_likelihood_df = data.frame(Iteration = iterations, LogLikelihood = log_likelihood_values)
  
  ggplot(log_likelihood_df, aes(x = Iteration, y = LogLikelihood)) +
    geom_line(color = "blue", size = 1) +
    geom_point(color = "red", size = 2) +
    labs(title = "EM Algorithm Log-Likelihood Convergence",
         x = "Iteration",
         y = "Log-Likelihood") +
    theme_minimal()
}

```

# EM function
The **EM function** was implemented following the algorithm described in *Pattern Recognition and Machine Learning* by **Christopher Bishop (2006, Chapter 9)**. The Expectation-Maximization (EM) algorithm iteratively estimates the parameters of a **Gaussian Mixture Model (GMM)** by maximizing the likelihood function. In the **E-step**, it computes the **responsibilities** $\gamma_{ik}$, which represent the probability that each data point $\mathbf{x}_i$ belongs to cluster $k$:

$$
\gamma_{ik} = \frac{\pi_k \mathcal{N}(\mathbf{x}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k)}
{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_i | \mathbf{\mu}_j, \mathbf{\Sigma}_j)}
$$

where $\pi_k$ is the **mixing coefficient**, $\mathbf{\mu}_k$ is the **mean**, and $\mathbf{\Sigma}_k$ is the covariance matrix of cluster $k$. In the M-step, the parameters are updated using the computed responsibilities. The new cluster means are estimated as:

$$
\mathbf{\mu}_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i
$$

where $N_k = \sum_{i=1}^{N} \gamma_{ik}$ represents the effective number of points assigned to cluster $k$. The covariance matrices are updated as:

$$
\mathbf{\Sigma}_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \mathbf{\mu}_k)(\mathbf{x}_i - \mathbf{\mu}_k)^T + \epsilon I
$$

where $\epsilon I$ is a small regularization term added for numerical stability, as suggested in *Bishop (2006, Section 9.2.2)*. The algorithm iterates until the log-likelihood function:

$$
\log L = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k) \right)
$$

converges. The function ensures an optimal and numerically stable implementation of EM for 1D, 2D, and high-dimensional data, including image segmentation.
## Function of EM for 1D
```{r}
EM_GMM_1D = function(X, K, max_iter = 100, tol = 1e-6) {
  N = length(X)  # n data
  
  set.seed(234) 
  mu = runif(K, min(X), max(X))  # Random means
  sigma = rep(var(X), K)  # Initialize  sample variance
  pi_k = rep(1/K, K)  # Equal mixture weights initially
  
  gamma = matrix(0, N, K)  # Responsibility matrix
  log_likelihood_values = numeric(max_iter)  # Store log-likelihood values
  
  log_likelihood = function() {
    sum(log(rowSums(sapply(1:K, function(k) {
      pi_k[k] * dnorm(X, mean = mu[k], sd = sqrt(sigma[k]))
    }))))
  }
  
  logL = log_likelihood()
  log_likelihood_values[1] = logL
  
  for (iter in 2:max_iter) {
    # E-step: Compute responsibilities
    for (k in 1:K) {
      gamma[, k] = pi_k[k] * dnorm(X, mean = mu[k], sd = sqrt(sigma[k]))
    }
    gamma = gamma / rowSums(gamma)
    
    # M-step: Update parameters
    Nk = colSums(gamma)  # Effective number of points in each cluster
    pi_k = Nk / N  # Update mixing coefficients
    mu = colSums(gamma * X) / Nk  # Update means
    sigma = colSums(gamma * (X - mu)^2) / Nk  # Update variances
    
    # Compute new log-likelihood
    new_logL = log_likelihood()
    log_likelihood_values[iter] = new_logL
    
    # Check for convergence
    if (abs(new_logL - logL) < tol) {
      message("Converged at iteration ", iter)
      log_likelihood_values = log_likelihood_values[1:iter]
      break
    }
    logL = new_logL
  }
  
  return(list(mu = mu, sigma = sigma, pi_k = pi_k, gamma = gamma, log_likelihood = log_likelihood_values))
}

```


```{r}
EM_GMM = function(X, K, max_iter = 100, tol = 1e-6) {
  # Check if X is a single-column matrix (1D case)
  if (ncol(X) == 1) {
    print("1D detected, calling EM_GMM_1D")
    return(EM_GMM_1D(as.vector(X), K, max_iter, tol))  # Convert to vector for 1D function
  }
  
  N = nrow(X)  # Number of data points
  D = ncol(X)  # Data dimensions
  
  params = initialize_em_parameters(X, K)
  mu = params$mu
  sigma = params$sigma
  pi_k = params$pi_k
  
  gamma = matrix(0, N, K)  # Responsibility matrix
  log_likelihood_values = numeric(max_iter)  # Store log-likelihood values
  
  log_likelihood = function() {
    sum(log(rowSums(sapply(1:K, function(k) {
      pi_k[k] * dmvnorm(X, mean = as.numeric(mu[k, , drop = FALSE]), sigma = sigma[, , k])
    }))))
  }
  
  logL = log_likelihood()
  log_likelihood_values[1] = logL  
  
  for (iter in 2:max_iter) {
    for (k in 1:K) {
      gamma[, k] = pi_k[k] * dmvnorm(X, mean = as.numeric(mu[k, , drop = FALSE]), sigma = sigma[, , k])
    }
    gamma = gamma / rowSums(gamma)
    
    Nk = colSums(gamma)
    pi_k = Nk / N
    mu = matrix(t(gamma) %*% X / Nk, ncol = D, byrow = TRUE)
    
    if (K == 1) {
      mu = matrix(mu, nrow = 1, ncol = D)
    }
    
    for (k in 1:K) {
      X_centered = sweep(X, 2, mu[k, , drop = FALSE], FUN = "-")
      sigma[, , k] = t(X_centered) %*% (X_centered * gamma[, k]) / Nk[k] + diag(1e-6, D)
    }
    
    new_logL = log_likelihood()
    log_likelihood_values[iter] = new_logL  
    
    if (abs(new_logL - logL) < tol) {
      message("Converged at iteration ", iter)
      log_likelihood_values = log_likelihood_values[1:iter]  
      break
    }
    logL = new_logL
  }
  
  return(list(mu = mu, sigma = sigma, pi_k = pi_k, gamma = gamma, log_likelihood = log_likelihood_values))
}
```
# Test for different dimensions

## 1D 
The first test is 1D this is a vector with 2 groups within the same vector.
```{r}
set.seed(123)
X_1D = matrix(c(rnorm(100, mean = -2, sd = 1), 
                 rnorm(100, mean = 3, sd = 1.5)), ncol = 1)

# Run EM Algorithm for 1D data
result_1D = EM_GMM(X_1D, K = 2)
  
# Corrected function call
plot_1D_2D_3D(X_1D, result_1D$gamma, result_1D$mu, result_1D$sigma)
```

# 2D 
We are testing this within the matrix with multivariable data of 2 different population
```{r warning=FALSE}

# Generate synthetic 2D data
library(MASS)
set.seed(123)
X_2D = rbind(
  mvrnorm(n = 100, mu = c(-2, -2), Sigma = matrix(c(1, 0.5, 0.5, 1), ncol = 2)),
  mvrnorm(n = 100, mu = c(3, 3), Sigma = matrix(c(1, -0.3, -0.3, 1), ncol = 2))
)

# Run EM Algorithm for 2D data
result_2D = EM_GMM(X_2D, K = 2)

# Plot the clustering results
plot_1D_2D_3D(X_2D, result_2D$gamma, result_2D$mu, result_2D$sigma)
```

#3D 
```{r}
true_means = matrix(c(5, 5, 5,  -5, -5, -5,  5, -5, 5), ncol = 3, byrow = TRUE)
true_covariances = list(
  diag(c(2, 1, 1)),  # Cluster 1
  diag(c(1, 2, 1)),  # Cluster 2
  diag(c(1, 1, 2))   # Cluster 3
)

# Generate data from 3 different Gaussians
X_list = lapply(1:3, function(k) {
  mvrnorm(n = 300 / 3, mu = true_means[k,], Sigma = true_covariances[[k]])
})

# Combine all generated data
X_3D = do.call(rbind, X_list)
colnames(X_3D) = c("V1", "V2", "V3")
em_results = EM_GMM(X_3D, K = 3)
# Extract fitted parameters
gamma_est = em_results$gamma
mu_est = em_results$mu
sigma_est = em_results$sigma

# -----------------------
# STEP 3: Plot using plot_GMM()
# -----------------------
plot_1D_2D_3D(X_3D, gamma_est, mu_est, sigma_est)
```

  
```{r warning=FALSE}
library(jpeg)
library(imager)

# Load and reshape the image
img = readJPEG("C:/Users/flore/Desktop/git/Statistical_learning/Test_alpha/20BT.jpg")
dim_img = dim(img)

# Reshape into N Ã— 3 (RGB) matrix
img_reshaped = cbind(as.vector(img[,,1]), as.vector(img[,,2]), as.vector(img[,,3]))

# Apply EM algorithm
result = EM_GMM(img_reshaped, K = 5)

reconstruct_image("C:/Users/flore/Desktop/git/Statistical_learning/Test_alpha/20BT.jpg", result)

```


