---
title: "Eigenfaces"
output: pdf_document
date: "2025-02-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE}
# library Imports
library(OpenImageR)
library(EBImage)
library(grid)
library(ggplot2)
library(caret)
library(pROC)
library(tictoc)
library(dplyr)
library(MASS)
library(gtools)   

```

# Notes and Teach inputs

Assignment 1
raw_data -> KNN
raw_data -> PCA -> KNN


Steps:
1. Intra Class distances (same person) -> Visualize resulting histograms
2. Between Class distances (different persons) 
3. Set Threshold between Distance BetweeenClasses and Distance InterClass
4. Use difference Distances
5. Use PCA 

Assignment 2
1. FDA
raw_data -> PCA -> DiscrAnal(Fisher Faces) -> KNN


TODOS: 
- Apply and compare PCA : Flo 
- Read Image Name into read_data output: Flo
- CV Classification Hyperparam tuning - Raw Data : Simon
- CV Classification Hyperparam tuning - PCA Data : Simon
- Classification Test with Images that are not in Database: Simon

- Apply and interprete FDA (PCA -> FDA): Flo
- PCA -> FDA -> KNN (Hyperparm): Simon

- Prep Code for prof test data: Simon

- Introduction, Conclusion, make pretty : Simon


Notes from Class

Hyperparm tuning

KNN (Dist, k, treshhold)
PCA - KNN (Dist, k,treshhold, expl var)
PCA - FDA - KNN (Dist, k, treshhold, expl var, number of fda's)

to avoid computational cost. Take optimal from previous and only optimize new parameter
JUSTIFY! 


# Introduction



# Data Import

```{r}

read_all_images <- function(folder_path) {
  image_files <- list.files(folder_path, full.names = TRUE, pattern = "\\.(jpg|png|jpeg|tiff|bmp)$", ignore.case = TRUE)
  
  # Sort filenames
  image_files <- mixedsort(image_files)  
  
  # Extract labels from filenames
  extract_label <- function(filename) {
    base_name <- tools::file_path_sans_ext(basename(filename))  
    label <- gsub("[^0-9]", "", base_name)  # Extract numeric part
    return(label)
  }
  extract_filename <- function(filepath) {
    return(basename(filepath))
  }
  read_data <- function(image_path) {
    img <- readImage(image_path)  
    red_aux   <- as.vector(img[,,1])
    green_aux <- as.vector(img[,,2])
    blue_aux  <- as.vector(img[,,3])
    
    # Combine all channels into a single row
    img_vector <- c(red_aux, green_aux, blue_aux)
    
    return(as.data.frame(t(img_vector)))  # Transpose to make it a row
  }
  
  image_list <- lapply(image_files, function(file) {
    img_data <- read_data(file)  
    img_data$Label <- extract_label(file)
    img_data$ID <- extract_filename(file) 
    return(img_data)
  })
  
  ax <- do.call(rbind, image_list)
  return(ax)
}
```




# Distance Histograms
```{r}
### Distance Histograms
distance_distributions <- function(data, distance_metric="euclidean"){
  
  ids <- data[, ncol(data)]
  
  # set up distance matrix/data frame
  dist_matrix <- as.matrix(dist(data[, -ncol(data)], method = distance_metric))
  dist_df <- data.frame(
    row1 = rep(1:nrow(data), each = nrow(data)),
    row2 = rep(1:nrow(data), times = nrow(data)),
    distance = as.vector(dist_matrix)
  )
  dist_df <- dist_df[dist_df$row1 != dist_df$row2, ] # remove self distances
  
  dist_df$id1 <- ids[dist_df$row1] # Assign IDs
  dist_df$id2 <- ids[dist_df$row2]
  
  dist_df_same_id <- dist_df[dist_df$id1 == dist_df$id2, ] # In group distances
  dist_df_same_id <- dist_df_same_id[order(dist_df_same_id$id1), ]
  
  
  dist_df_diff_id <- dist_df[dist_df$id1 != dist_df$id2, ] # Outside group distances
  
  dist_df_diff_id$id1 = 0
  dist_df_diff_id$id2 = 0
  
  dist_df_split <- rbind(dist_df_same_id,dist_df_diff_id)
  
  
  # Plot histogram
  plot = ggplot(dist_df_split, aes(x = distance, fill = factor(id1))) +
    geom_density(alpha = 0.6, position = "identity") +
    labs(title = "Histogram of Pairwise Distances by ID",
         x = "Distance", y = "Frequency", fill = "ID Group") +
    theme_minimal()
  
  return(plot)
    
}

```




# Principal Component Analysis (PCA)

The following section contains a function to perform principal Component Analysis on any kind of Data. The function is first defined and then applied to the image data and its results are compared to the base R pca function.

```{r}
### PCA Function
PCA.fun = function(X){
  X = X %>% dplyr::select(-c("Label","ID"))
  print(dim(X))
  X = as.matrix(X)
  X = apply(X, 2, as.numeric) # make sure it is numeric
  n = nrow(X)
  mu = colMeans(X)
  # center data
  G = sweep(X, 2, mu, FUN = "-")
  # Compute the covariance matrix
  cov_matrix =(1/(n-1))*(G %*% t(G))
  ## Calculate Eigenvalues and Eigenvectors and sort
  eig = eigen(cov_matrix)
  eig_val = eig$values
  eig_vec = eig$vectors
  ei_vec_large = t(G)%*%eig_vec
  # sort
  sort_index <- order(eig_val, decreasing = TRUE)
  eig_val_sorted <- eig_val[sort_index]
  eig_vec_sorted <- ei_vec_large[, sort_index] 
  #variability of each PC
  D = eig_val_sorted/sum(eig_val_sorted)
  return(list("Eigen Vector"= eig_vec_sorted,"D"=D))
  }
```
This function is created using the following formula. 
$$\Sigma_{s}= \frac{1}{n-1}GG^{t}$$
Where G is the matrix of features less the mean of each column.
The eigen vectors of this new matrix are the same eigen vector of the original matrix and if you multiply $G^{t}\cdot eigen\_vector$ you will get the eigen vector of the typical matrix.

```{r}
folder_path = "Training"
data_PCA = read_all_images(folder_path)
print(system.time(PCA.fun(data_PCA)))
PCA_result = PCA.fun(data_PCA)
cumsum(PCA_result$D)
data_PC = as.matrix(data_PCA[,1:108000])%*%PCA_result$`Eigen Vector`[,1:40]
print(system.time(prcomp(as.matrix(data_PCA[,1:108000]),scale. = T)))
compare_pc = prcomp(as.matrix(data_PCA[,1:108000]),scale. = T)
print(cumsum(compare_pc$sdev^2)/sum(compare_pc$sdev^2))

```

If we compare the PCA function we created with prcomp(), we can see a significant difference in execution time and system resource usage. This is because the base function assumes that $n >> p$, but for the type of data we are working with, this approach is less optimal. Instead, we use a function that is optimized for cases where $n <<p$.

When comparing the eigenvalues, our custom function required fewer eigenvalues to achieve the same explained variance.

```{r}
df <- data.frame(
  Components = 1:length(PCA_result$D),
  CumulativeVariance = cumsum(PCA_result$D)
)

# Plot the cumulative explained variance
ggplot(df, aes(x = Components, y = CumulativeVariance)) +
  geom_line() +
  geom_point() +
  labs(title = "Cumulative Explained Variance",
       x = "Principal Components",
       y = "Cumulative Variance Explained") +
  theme_minimal()
```

In the graph you can see that if we want to get close to 90% of the variance of the data we only need to get 25 principal components.

# Fisher Discriminant Analysis (FDA)

```{r}
FDA.fun = function(data, labels, reg = 1e-6) {
  data = as.matrix(data)
  labels = as.factor(labels)
  
  num_classes = length(unique(labels))
  num_features = ncol(data)
  #overall mean
  mean_total = colMeans(data)
  #mean by class
  class_levels = levels(labels)
  class_means = matrix(0, nrow = num_classes, ncol = num_features)
  rownames(class_means) = class_levels
  
  for (i in 1:num_classes) {
    class_means[i, ] = colMeans(data[labels == class_levels[i], , drop = FALSE])
  }
  #within classes matrix
  Sw = matrix(0, num_features, num_features)
  for (i in 1:num_classes) {
    class_data = data[labels == class_levels[i], , drop = FALSE]
    mean_diff = sweep(class_data, 2, class_means[i, ], "-")  
    Sw = Sw + t(mean_diff) %*% mean_diff
  }
  
  Sw = Sw + reg * diag(num_features)
  
  Sb = matrix(0, num_features, num_features)
  #between classes matrix
  for (i in 1:num_classes) {
    mean_diff = matrix(class_means[i, ] - mean_total, ncol = 1)
    Sb = Sb + sum(labels == class_levels[i]) * (mean_diff %*% t(mean_diff))
  }
  
  L = chol(Sw)
  L_inv = solve(L)
  S_transformed = t(L_inv) %*% Sb %*% L_inv
  
  eig = eigen(S_transformed)
  idx = order(Re(eig$values), decreasing = TRUE)
  eigenvalues = Re(eig$values[idx])
  P = L_inv %*% Re(eig$vectors[, idx])
  
  num_components = min(num_classes - 1, num_features)
  eigenvalues = eigenvalues[1:num_components]
  P = P[, 1:num_components, drop = FALSE]
  D = eigenvalues / sum(eigenvalues)  
  transform_data = data %*% P
  return(list(mean = mean_total, P = P, D = D,transform_data= transform_data,labels=labels))
}
```


This function implements Fisher Discriminant Analysis to find a projection matrix $P$ that maximizes class separability. It computes the within-class scatter matrix $S_{w}$ and between-class scatter matrix $ S_{b} $, then solves the generalized eigenvalue problem for $ S_{w}^{-1} S_{b} $. Since $ S_{w}$ can be singular or ill-conditioned, direct inversion is unstable. Instead, we use Cholesky decomposition $ S_{w} = L L^{T} $ to transform the problem into a standard eigenvalue decomposition, ensuring numerical stability. The function returns the mean vector $ \mu$, the projection matrix $ P $ (top eigenvectors), and the variance explained $D$ by each discriminant.

```{r}
pca_result <- PCA.fun(data_PCA)

# Get results and get PC's that explain 95% Variance
eig_vecs <- pca_result$"Eigen Vector"
cumulative_variance <- cumsum(pca_result$D)
num_components <- which(cumulative_variance >= 0.95)[1]

# Project data onto selected PCs
data_reduced <- as.matrix(data_PCA %>% dplyr::select(-c("Label","ID"))) %*% eig_vecs[, 1:num_components]


result_FDA = FDA.fun(data_reduced,data_PCA$Label)

data_reduced_FDA <-result_FDA$transform_data
df_plot <- data.frame(result_FDA$transform_data, labels = result_FDA$labels)
ggplot(df_plot, aes(x = X1, y = X2, color = labels)) + geom_point()

```

```{r}
df_FDA <- data.frame(
  Components = 1:length(result_FDA$D),
  CumulativeVariance = cumsum(result_FDA$D)
)

# Plot the cumulative explained variance
ggplot(df_FDA, aes(x = Components, y = CumulativeVariance)) +
  geom_line() +
  geom_point() +
  labs(title = "Cumulative Explained Variance",
       x = "Fisher Discriminants",
       y = "Cumulative Variance Explained") +
  theme_minimal()
```
If you take the first 10 fisher discriminate you can get 90% of the reduce (PCA) matrix variability explain.
```{r}
system.time(FDA.fun(data_reduced,data_PCA$Label))
system.time( MASS::lda(data_reduced,data_PCA$Label))
```
We obtain the same results meaning that or function avoid problems and can be use as the one in MASS library.


## Functions

### Test/Train Split

```{r}
# TODO: Test/Train Split
train_test_split <- function(data, train_ratio = 0.8, seed = NULL) {
  if (!is.null(seed)) {
    set.seed(seed)
  }
  # Split
  n = nrow(data)
  train_indices <- sample(1:n, size = floor(train_ratio * n))
  train_data <- data[train_indices, , drop = FALSE]  # Train set
  test_data <- data[-train_indices, , drop = FALSE]  # Test set

  return(list(train = train_data, test = test_data))
}
```

### KNN 

```{r}
### Distances

# Euclidean distance
euclidean_distance <- function(x, y) {
  sqrt(sum((x - y)^2))
}

# Manhattan distance
manhattan_distance <- function(x, y) {
  sum(abs(x - y))
}

```

```{r}
### Threshhold functions

## Criterion Functions
# precentile threshold function
estimate_threshold_via_percentile <- function(data, percentile = 0.90) {
  dist_1 <- as.matrix(dist(data))
  threshold <- quantile(dist_1[upper.tri(dist_1)], percentile)
  return(threshold)
}

## generate Treshhold functions
estimate_thresholds <- function(train_data, estimate_func, percentile) {
  thresholds <- numeric()
  train_data_ids_unique <- unique(train_data[, ncol(train_data)])
  
  for (id in train_data_ids_unique) {
    train_data_grouped <- train_data[train_data[, ncol(train_data)] == id, , drop = FALSE]
    if (nrow(train_data_grouped) > 1) {
      thresholds[id] <- estimate_func(train_data_grouped,percentile = percentile)
    }
  }
  
  return(thresholds)
}
```


```{r}
### KNN Classifier
knn_classifier = function(test_data, train_data, thresholds, dist_metric = "euclidean", k, knn_bool = T){
  # init results array
  predicted_ids = c() 
  
  # for all rows (images) run
  for (i in 1:nrow(test_data)) {
    
    ### Prepare Distances
    # For train data, get true ID and drop id col
    train_data_ids <- train_data[, ncol(train_data)]  # Extract ID column
    train_data_no_last_col <- train_data[, -ncol(train_data), drop = FALSE]
    
    # for test data, get row
    test_row = test_data[i,]
    
    # Compute distances between the test row and each row in the training data and assign IDs
    distances <- apply(train_data_no_last_col, 1, function(train_row) {
      test_row <- as.numeric(test_row)
      train_row <- as.numeric(train_row)
      
      if (dist_metric == "euclidean") {
        return(euclidean_distance(test_row, train_row))
      } else if (dist_metric == "manhattan") {
        return(manhattan_distance(test_row, train_row))
      }
    })
    names(distances) <- train_data_ids
  
    ### Classification
    predicted_label <- "0"
    # fot all groups IDs, check if threshhold is passed for at least one
    for (train_ids in names(thresholds)) { 
      # perform knn as soon as distance threshhod is passed for one
      if (any(distances[names(distances) == train_ids] < thresholds[train_ids])) { 
        if (knn_bool){
          k_neighbors <- order(distances)[1:min(k, length(distances))] # with k >= training points
          neighbor_labels <- train_data_ids[k_neighbors]  # Get labels of k-nearest neighbors
          
          # Get most frequent label
          predicted_label <- names(sort(table(neighbor_labels), decreasing = TRUE))[1]
          
          # store result
          predicted_ids = c(predicted_ids, predicted_label)
          break  # Stop once a group match is found
          
        } else{ # in case no KNN and only checking if in data or not 
          predicted_label = 1
          predicted_ids = c(predicted_ids, predicted_label)
          break
        }
      }
    }
    # if not passing any threshholds, assign group 0 (no group)
    if (predicted_label == "0") {
        predicted_ids = c(predicted_ids, predicted_label)
    }
  }
  
  # return predictions
  return(predicted_ids)
}
```


```{r}
### Scoring
score <- function(df_id,df_binary) {
  
  ### ID Classification Scoring
  
  # match & fail rate 
  match_rate <- mean(df_id$true == df_id$predicted)
  fail_rate <- 1 - match_rate
  
  ### Binary Classification Scoring
  # factorize
  df_binary$true = factor(df_binary$true, levels = c(0, 1))
  df_binary$predicted = factor(df_binary$predicted, levels = c(0, 1))

  # confusion with case of only values either 1 or 0
  unique_vals <- unique(c(df_binary$true, df_binary$predicted))
  if (all(unique_vals == 1) | all(unique_vals == 0)){
    return(list(binary_false_positive_rate = 0, 
              binary_false_negative_rate = 0,
              classification_match_rate = match_rate,
              classification_fail_rate = fail_rate))
  } else {
    conf_matrix <- table(df_binary$true, df_binary$predicted)
  }
  
  # Extract values
  TP <- conf_matrix[2, 2]  
  TN <- conf_matrix[1, 1]  
  FP <- conf_matrix[1, 2]  
  FN <- conf_matrix[2, 1]
  
  # Compute Metrics
  type1_error <- ifelse((FP + TN) == 0, 0, FP / (FP + TN))
  type2_error <- ifelse((FN + TP) == 0, 0, FN / (FN + TP))
  
  
  return(list(binary_false_positive_rate = type1_error, 
              binary_false_negative_rate = type2_error,
              classification_match_rate = match_rate,
              classification_fail_rate = fail_rate))
}
```


```{r}
### Pipeline
classification_pipeline <- function(train_data,
                                    test_data,
                                    estimate_func,
                                    percentile,
                                    knn_k,
                                    dist_metric,
                                    expl_var_pca = 0.95,
                                    expl_var_fda = 0.95,
                                    knn_bool = T,
                                    pca_bool = F,
                                    fda_bool =F) {
  
  # Replace IDs in test_data that are not in train_data with "0"
  train_ids <- train_data[, ncol(train_data)]
  test_ids <- test_data[, ncol(test_data)]
  test_data[, ncol(test_data)][!test_ids %in% train_ids] <- 0
  
  
  # run pca if required 
  if (pca_bool){
    print("######## Run PCA ########")
    
    # turn to df for compatability
    train_data <- as.data.frame(train_data[,-ncol(train_data)])
    train_data <- train_data %>% mutate_all(as.numeric)
    
    train_data$Label <- NA
    train_data$ID <- NA
    
    # Run PCA
    pca_result <- PCA.fun(train_data)
    
    # Get results and get PC's that explain 95% Variance
    eig_vecs <- pca_result$"Eigen Vector"
    cumulative_variance <- cumsum(pca_result$D)
    num_components_pca <- which(cumulative_variance >= expl_var_pca)[1]
    
    # Project data onto selected PCs
    train_data <- as.matrix(train_data %>% dplyr::select(-Label)) %*% eig_vecs[, 1:num_components_pca]
    test_data <- as.matrix(test_data %>% dplyr::select(-Label)) %*% eig_vecs[, 1:num_components_pca]

    train_data = cbind(train_data, train_ids)
    test_data = cbind(test_data, test_ids)

  }
  
  if (fda_bool){
    print("######## Run FDA ########")
    
    # turn to df for compatability
    train_data <- as.data.frame(train_data[,-ncol(train_data)])
    train_data <- train_data %>% mutate_all(as.numeric)

    train_data$Label <- NA
    train_data$ID <- NA
    
    # run fda
    result_FDA = FDA.fun(train_data[,-ncol(train_data)],train_ids)
    
    # select discriminants according to explained variance
    cumulative_variance_FDA <- cumsum(result_FDA$D)
    num_components_fda <- which(cumulative_variance_FDA >= expl_var_fda)[1]
    
    # transform train & test
    train_data = as.matrix(train_data)
    train_data = train_data[,-ncol(train_data)]%*% result_FDA$P[, 1:num_components_fda]
    test_data = test_data[,-ncol(test_data)]%*% result_FDA$P[, 1:num_components_fda]
  
    train_data = cbind(train_data, train_ids)
    test_data = cbind(test_data, test_ids)
    
  }

  
  # Estimate thresholds
  thresholds <- estimate_thresholds(train_data, estimate_func = estimate_func, percentile = percentile)
  
  # Prepare test data
  true_test_ids <- c(test_data[, ncol(test_data), drop = FALSE])
  test_data_input <- test_data[, -ncol(test_data), drop = FALSE]
  
  # Run KNN classifier
  predicted_ids <- knn_classifier(test_data_input, train_data, thresholds = thresholds, dist_metric = dist_metric,k = knn_k,knn_bool = knn_bool)
  
  
  # Merge results
  df_classification <- data.frame(
    true = as.numeric(true_test_ids),
    predicted = as.numeric(predicted_ids)
  )
  
  # Derive binary classification [0,1] (if in data set)
  df_classification_binary <- as.data.frame(lapply(df_classification, function(x) ifelse(x == 0, 0, 1)))
  
  # Calculate score
  scores <- score(df_id = df_classification, df_binary = df_classification_binary)
  
  # Return results
  return(list(
    classification = df_classification,
    classification_binary = df_classification_binary,
    binary_false_positive_rate = scores$binary_false_positive_rate, 
    binary_false_negative_rate = scores$binary_false_negative_rate,
    classification_match_rate = scores$classification_match_rate,
    classification_fail_rate = scores$classification_fail_rate
  ))
}

```




```{r}
### K fold Cross Validation
knn_k_fold_cv <- function(data,
                          folds,
                          k_CV,
                          estimate_func, 
                          percentile, 
                          knn_k, 
                          dist_metric,
                          expl_var_pca,
                          expl_var_fda,
                          knn_bool,
                          pca_bool,
                          print_bool = F) {

  all_results <- list()
  # Perform K-fold cross-validation
  for(i in 1:k_CV) {
    
    # Split data into training and testing sets based on the fold
    train_data <- data[folds != i, ]
    test_data <- data[folds == i, ]
    
    # Run the classification pipeline
    results <- classification_pipeline(train_data, 
                                       test_data,
                                       estimate_func = estimate_func,
                                       percentile = percentile,
                                       knn_k = knn_k ,
                                       dist_metric = dist_metric,
                                       expl_var_pca,
                                       expl_var_fda,
                                       knn_bool,
                                       pca_bool)
    
    # Store results for this fold
    all_results[[i]] <- list(
      binary_false_positive_rate = results$binary_false_positive_rate,
      binary_false_negative_rate = results$binary_false_negative_rate,
      classification_match_rate = results$classification_match_rate,
      classification_fail_rate = results$classification_fail_rate
    )
  }
  
  # Store means
  binary_false_positive_rate_mean <- mean(sapply(all_results, function(x) x$binary_false_positive_rate))
  binary_false_negative_rate_mean <- mean(sapply(all_results, function(x) x$binary_false_negative_rate))
  classification_match_rate_mean <- mean(sapply(all_results, function(x) x$classification_match_rate))
  classification_fail_rate_mean <- mean(sapply(all_results, function(x) x$classification_fail_rate))

  avg_scores = list(binary_false_positive_rate_mean = binary_false_positive_rate_mean,
                     binary_false_negative_rate_mean = binary_false_negative_rate_mean,
                     classification_match_rate_mean = classification_match_rate_mean,
                     classification_fail_rate_mean = classification_fail_rate_mean)
  
  # Print all average metrics
  if (print_bool){
    print("Average scores across all folds:")
    print(avg_scores)
  }

  return(avg_scores)#, scores_by_fold=all_results))
}

# Example of using the function
#results <- k_fold_cv(data_matriz, k_CV = 4)
```


```{r}
CV_grid_search <- function(data, 
                           knn_k_values, 
                           percentile_values, 
                           dist_metrics,
                           expl_var_pca_grid,
                           expl_var_fda_grid,
                           k_CV = 5,
                           knn_bool = F,
                           pca_bool = F) {
  # Store results
  results_list <- list()
  
  folds <- sample(1:k_CV, nrow(data), replace = TRUE)

  # Perform grid search
  for (knn_k in knn_k_values) {
    for (percentile in percentile_values) {
      for (dist_metric in dist_metrics) {
        for (expl_var_pca in expl_var_pca_grid) {
          for (expl_var_fda in expl_var_fda_grid) {
            # Run cross-validation
            avg_scores <- knn_k_fold_cv(data,
                                        folds,
                                        k_CV = k_CV,
                                        estimate_func = estimate_threshold_via_percentile,
                                        percentile = percentile,
                                        knn_k = knn_k,
                                        dist_metric = dist_metric,
                                        expl_var_pca = expl_var_pca,
                                        expl_var_fda = expl_var_fda,
                                        knn_bool,
                                        pca_bool)
        
            # Store results
            results_list[[paste(knn_k, percentile, dist_metric, sep="_")]] <- list(
              knn_k = knn_k,
              percentile = percentile,
              dist_metric = dist_metric,
              expl_var_pca = expl_var_pca,
              expl_var_fda = expl_var_fda,
              binary_false_positive_rate_mean = avg_scores$binary_false_positive_rate_mean,
              binary_false_negative_rate_mean = avg_scores$binary_false_negative_rate_mean,
              classification_match_rate_mean = avg_scores$classification_match_rate_mean,
              classification_fail_rate_mean = avg_scores$classification_fail_rate_mean
            )
          }
        }
      }
    }
  }
  
  # Convert results to data frame
  results_df <- do.call(rbind, lapply(results_list, as.data.frame))
  
  # Find the best parameters for threshold and for knn
  best_params_knn <- results_df[which.max(results_df$classification_match_rate_mean), ]
  best_params_binary <- results_df[which.min(results_df$binary_false_negative_rate_mean), ]
  
  print("#################### Best Hyperparameters ####################")
  print(paste("Distance Metric: ", best_params_knn$dist_metric))
  print(paste("Percentile for Quantile Threshold: ", best_params_binary$percentile))
  print(paste("KNN K nearest neighbors: ", best_params_knn$knn_k))
  print(paste("PCA explained Variance Threshhold: ", best_params_knn$expl_var_pca))
  print(paste("FDA explained Variance Threshhold: ", best_params_knn$expl_var_fda))


  return(list(results_df = results_df, 
              best_params_knn = best_params_knn, 
              best_params_binary = best_params_binary))
}
```





```{r}
### Predict for new data
knn_classifier_full <- function(path_train, 
                           path_test,                               
                           dist_metric,
                           knn_k,
                           result, 
                           estimate_func = estimate_threshold_via_percentile,
                           percentile = percentile,
                           pca_bool = F) {

  

  
  data_train = read_all_images(path_train)
  data_test = read_all_images(path_test)
  
  image_names_test = data_test$ID
  
  data_train_labels <- data_train$Label
  data_test_labels <- data_test$Label  

  data_train = data_train %>% dplyr::select(-ID)
  data_test = data_test %>% dplyr::select(-ID)


  
  if (pca_bool){
    
    pca_result <- PCA.fun(data_train)
    
    # Get results and get PC's that explain 95% Variance
    eig_vecs <- pca_result$"Eigen Vector"
    cumulative_variance <- cumsum(pca_result$D)
    num_components <- which(cumulative_variance >= 0.95)[1]
    
    # Project data onto selected PCs
    data_train_asmatrix <- as.matrix(data_train %>% dplyr::select(-Label)) %*% eig_vecs[, 1:num_components]
    data_test_asmatrix <- as.matrix(data_test %>% dplyr::select(-Label)) %*% eig_vecs[, 1:num_components]

    data_train_asmatrix = cbind(data_train_asmatrix, data_train_labels)
    data_test_asmatrix = cbind(data_test_asmatrix, data_test_labels)
    
    print(data_test_asmatrix[,ncol(data_test_asmatrix)])

  }else{

      
    data_train_asmatrix = as.matrix(data_train)
    data_test_asmatrix = as.matrix(data_test)
    
    print(data_test_asmatrix[,ncol(data_test_asmatrix)])
  }
  
  
  classification_result = classification_pipeline(train_data = data_train_asmatrix,
                                    test_data = data_test_asmatrix,
                                    estimate_func = estimate_func,
                                    percentile = percentile,
                                    knn_k = knn_k,
                                    dist_metric = dist_metric)
  
  classification_result$classification = cbind(image_names_test,classification_result$classification)
  classification_result$classification_binary = cbind(image_names_test,classification_result$classification_binary)

  
  # Return result
  return(classification_result)
}
```


# Run Codes

## Load Data
```{r}
folder_path = "Training_alpha"
data = read_all_images(folder_path)

labels <- data$Label  
image_names = data$ID

data = data %>% dplyr::select(-ID)

data_matriz = as.matrix(data)
```


## Distance Histograms

```{r}
# Run
for (distance_metric in c("euclidean", "manhattan")) {
  hist = distance_distributions(data_matriz, distance_metric = distance_metric)
  print(hist)
}

```

## Run KNN


### Run KNN on Raw Data

```{r}
### Run grid Search to optimize Hyperparameters
CV_grid_search_result <- CV_grid_search(data_matriz,
                                        k_CV = 5,
                                        knn_k_values = c(3, 4, 5, 6, 7), 
                                        percentile_values = c(0.8, 0.9, 0.95, 0.99), 
                                        dist_metrics = c("euclidean", "manhattan"),
                                        expl_var_pca_grid = c(0.9,0.95),
                                        expl_var_fda_grid = c(0.9,0.95),
                                        knn_bool = T,
                                        pca_bool = T
                                        )

#print(CV_grid_search_result$results_df)
print(CV_grid_search_result$best_params_knn)
#print(CV_grid_search_result$best_params_binary)

```


### Run KNN on PCA Data


```{r}
### Run PCA & prep data
pca_result <- PCA.fun(data)

# Get results and get PC's that explain 95% Variance
eig_vecs <- pca_result$"Eigen Vector"
cumulative_variance <- cumsum(pca_result$D)
num_components <- which(cumulative_variance >= 0.95)[1]

# Project data onto selected PCs
data_reduced <- as.matrix(data %>% dplyr::select(-Label)) %*% eig_vecs[, 1:num_components]
data_reduced_labeled = cbind(data_reduced, labels)

```


```{r}
### Run grid Search to optimize Hyperparameters
CV_grid_search_result <- CV_grid_search(data_reduced_labeled,
                                        k_CV = 5,
                                        knn_k_values = c(3, 4, 5, 6, 7), 
                                        percentile_values = c(0.8, 0.9, 0.95, 0.99), 
                                        dist_metrics = c("euclidean", "manhattan"))

#print(CV_grid_search_result$results_df)
print(CV_grid_search_result$best_params_knn)
print(CV_grid_search_result$best_params_binary)

```


```{r}

```


### Run KNN on data dimension reduced by FDA and PCA

```{r}
df_FDA <- data.frame(
  Components = 1:length(result_FDA$D),
  CumulativeVariance = cumsum(result_FDA$D)
)
```




### Compare Results

TODO: Discuss Results

```{r}
# TODO: Display Results
```




## Run KNN for new Data on best Combination

```{r}
### Predict for new Data
results = knn_classifier_full( path_train = "Training_alpha", 
                               path_test = "Test_alpha",
                               dist_metric = 'euclidean',
                               knn_k = 3,
                               estimate_func = estimate_threshold_via_percentile,
                               percentile = 0.99,
                               pca_bool = T)

results
```





















### Compare Results














