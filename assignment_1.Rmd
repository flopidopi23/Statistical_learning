---
title: "Eigenfaces"
output: pdf_document
date: "2025-02-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(OpenImageR)
library(EBImage)
library(grid)
library(ggplot2)
library(caret)
library(pROC)

```

# Notes and Teach inputs

Assignment 1
raw_data -> KNN
raw_data -> PCA -> KNN

Steps:
1. Intra Class distances (same person) -> Visualize resulting histograms
2. Between Class distances (different persons) 
3. Set Threshold between Distance BetweeenClasses and Distance InterClass
4. Use difference Distances
5. Use PCA 

Assignment 2
1. FDA
raw_data -> PCA -> DiscrAnal(Fisher Faces) -> KNN

# Data Import and Preprocessing 

```{r}
# load selfmade functions
source("PCA_function.R")
source("read_data_function.R")
```


# Data Import and Preprocessing 

```{r}
folder_path = "Training"
data = read_all_images(folder_path)
data_matriz = as.matrix(data)

```


# PCA

```{r}
### Verify with library
#out = princomp(X)

#pc1 = matrix(out$scores[,1],nrow=nrow(red), ncol=ncol(green))
#imageShow(pc1)

#pc2 = matrix(out$scores[,2],nrow=nrow(red), ncol=ncol(green))
#imageShow(pc2)

#pc3 = matrix(out$scores[,3],nrow=nrow(red), ncol=ncol(green))
#imageShow(pc3)
```






### Distance Histograms


```{r}

distance_distributions <- function(data, distance_metric="euclidean"){
  
  ids <- data[, ncol(data)]
  
  # set up distance matrix/data frame
  dist_matrix <- as.matrix(dist(data[, -ncol(data)], method = distance_metric))
  dist_df <- data.frame(
    row1 = rep(1:nrow(data), each = nrow(data)),
    row2 = rep(1:nrow(data), times = nrow(data)),
    distance = as.vector(dist_matrix)
  )
  dist_df <- dist_df[dist_df$row1 != dist_df$row2, ] # remove self distances
  
  dist_df$id1 <- ids[dist_df$row1] # Assign IDs
  dist_df$id2 <- ids[dist_df$row2]
  
  dist_df_same_id <- dist_df[dist_df$id1 == dist_df$id2, ] # In group distances
  dist_df_same_id <- dist_df_same_id[order(dist_df_same_id$id1), ]
  
  
  dist_df_diff_id <- dist_df[dist_df$id1 != dist_df$id2, ] # Outside group distances
  
  dist_df_diff_id$id1 = 0
  dist_df_diff_id$id2 = 0
  
  dist_df_split <- rbind(dist_df_same_id,dist_df_diff_id)
  
  
  # Plot histogram
  plot = ggplot(dist_df_split, aes(x = distance, fill = factor(id1))) +
    geom_density(alpha = 0.6, position = "identity") +
    labs(title = "Histogram of Pairwise Distances by ID",
         x = "Distance", y = "Frequency", fill = "ID Group") +
    theme_minimal()
  
  return(plot)
    
}

```

```{r}
# Run
for (distance_metric in c("euclidean", "manhattan")) {
  hist = distance_distributions(data_matriz, distance_metric = distance_metric)
  print(hist)
}

```

## Functions

### Test/Train Split

```{r}
# TODO: Test/Train Split
train_test_split <- function(data, train_ratio = 0.8, seed = NULL) {
  if (!is.null(seed)) {
    set.seed(seed)
  }
  # Split
  n = nrow(data)
  train_indices <- sample(1:n, size = floor(train_ratio * n))
  train_data <- data[train_indices, , drop = FALSE]  # Train set
  test_data <- data[-train_indices, , drop = FALSE]  # Test set

  return(list(train = train_data, test = test_data))
}
```

### KNN 

```{r}
### Distances

# Euclidean distance
euclidean_distance <- function(x, y) {
  sqrt(sum((x - y)^2))
}

# Manhattan distance
manhattan_distance <- function(x, y) {
  sum(abs(x - y))
}

```

```{r}
### Threshhold functions

## Criterion Functions
# precentile threshold function
estimate_threshold_via_percentile <- function(data, percentile = 0.90) {
  dist_1 <- as.matrix(dist(data))
  threshold <- quantile(dist_1[upper.tri(dist_1)], percentile)
  return(threshold)
}

## generate Treshhold functions
estimate_thresholds <- function(train_data, estimate_func) {
  thresholds <- numeric()
  train_data_ids_unique <- unique(train_data[, ncol(train_data)])
  
  for (id in train_data_ids_unique) {
    train_data_grouped <- train_data[train_data[, ncol(train_data)] == id, , drop = FALSE]
    if (nrow(train_data_grouped) > 1) {
      thresholds[id] <- estimate_func(train_data_grouped)
    }
  }
  
  return(thresholds)
}
```


```{r}
### KNN Classifier
knn_classifier = function(test_data, train_data, thresholds, dist_metric = "euclidean", k){
  # init results array
  predicted_ids = c() 
  
  # for all rows (images) run
  for (i in 1:nrow(test_data)) {
    
    ### Prepare Distances
    # For train data, get true ID and drop id col
    train_data_ids <- train_data[, ncol(train_data)]  # Extract ID column
    train_data_no_last_col <- train_data[, -ncol(train_data), drop = FALSE]
    
    # for test data, get row
    test_row = test_data[i,]
    
    # Compute distances between the test row and each row in the training data and assign IDs
    distances <- apply(train_data_no_last_col, 1, function(train_row) {
      test_row <- as.numeric(test_row)
      train_row <- as.numeric(train_row)
      
      if (dist_metric == "euclidean") {
        return(euclidean_distance(test_row, train_row))
      } else if (dist_metric == "manhattan") {
        return(manhattan_distance(test_row, train_row))
      }
    })
    names(distances) <- train_data_ids
  
    ### Classification
    predicted_label <- "0"
    # fot all groups IDs, check if threshhold is passed for at least one
    for (train_ids in names(thresholds)) { 

      # perform knn as soon as distance threshhod is passed for one
      if (any(distances[names(distances) == train_ids] < thresholds[train_ids])) { 

        k_neighbors <- order(distances)[1:min(k, length(distances))] # with k >= training points
        neighbor_labels <- train_data_ids[k_neighbors]  # Get labels of k-nearest neighbors
        
        # Get most frequent label
        predicted_label <- names(sort(table(neighbor_labels), decreasing = TRUE))[1]
        
        # store result
        predicted_ids = c(predicted_ids, predicted_label)
        break  # Stop once a group match is found
      }
    }
    # if not passing any threshholds, assign group 0 (no group)
    if (predicted_label == "0") {
        predicted_ids = c(predicted_ids, predicted_label)
    }
  }
  
  # return predictions
  return(predicted_ids)
}
```


```{r}
### Scoring
score <- function(df_id,df_binary) {
  
  ### ID Classification Scoring
  
  # match & fail rate 
  match_rate <- mean(df_id$true == df_id$predicted)
  fail_rate <- 1 - match_rate
  
  ### Binary Classification Scoring
  # factorize
  df_binary$true = factor(df_binary$true, levels = c(0, 1))
  df_binary$predicted = factor(df_binary$predicted, levels = c(0, 1))

  # confusion with case of only values either 1 or 0
  unique_vals <- unique(c(df_binary$true, df_binary$predicted))
  if (all(unique_vals == 1) | all(unique_vals == 0)){
    return(list(binary_false_positive_rate = 0, 
              binary_false_negative_rate = 0,
              classification_match_rate = match_rate,
              classification_fail_rate = fail_rate))
  } else {
    conf_matrix <- table(df_binary$true, df_binary$predicted)
  }
  
  # Extract values
  TP <- conf_matrix[2, 2]  
  TN <- conf_matrix[1, 1]  
  FP <- conf_matrix[1, 2]  
  FN <- conf_matrix[2, 1]
  
  # Compute Metrics
  type1_error <- ifelse((FP + TN) == 0, 0, FP / (FP + TN))
  type2_error <- ifelse((FN + TP) == 0, 0, FN / (FN + TP))
  
  
  return(list(binary_false_positive_rate = type1_error, 
              binary_false_negative_rate = type2_error,
              classification_match_rate = match_rate,
              classification_fail_rate = fail_rate))
}
```


```{r}
### Pipeline
classification_pipeline <- function(train_data,
                                    test_data,
                                    estimate_func = estimate_threshold_via_percentile,
                                    knn_k = 5,
                                    dist_metric = "euclidean") {
  
  # Replace IDs in test_data that are not in train_data with "0"
  train_ids <- train_data[, ncol(train_data)]
  test_ids <- test_data[, ncol(test_data)]
  test_data[, ncol(test_data)][!test_ids %in% train_ids] <- 0
  
  # Estimate thresholds
  thresholds <- estimate_thresholds(train_data, estimate_func = estimate_func)
  
  # Prepare test data
  true_test_ids <- c(test_data[, ncol(test_data), drop = FALSE])
  test_data_input <- test_data[, -ncol(test_data), drop = FALSE]
  
  # Run KNN classifier
  predicted_ids <- knn_classifier(test_data_input, train_data, thresholds = thresholds, dist_metric = dist_metric,k = knn_k)
  
  # Merge results
  df_classification <- data.frame(
    true = as.numeric(true_test_ids),
    predicted = as.numeric(predicted_ids)
  )
  
  # Derive binary classification [0,1] (if in data set)
  df_classification_binary <- as.data.frame(lapply(df_classification, function(x) ifelse(x == 0, 0, 1)))
  
  # Calculate score
  scores <- score(df_id = df_classification, df_binary = df_classification_binary)
  
  # Return results
  return(list(
    classification = df_classification,
    classification_binary = df_classification_binary,
    binary_false_positive_rate = scores$binary_false_positive_rate, 
    binary_false_negative_rate = scores$binary_false_negative_rate,
    classification_match_rate = scores$classification_match_rate,
    classification_fail_rate = scores$classification_fail_rate
  ))
}

```




```{r}
### K fold Cross Validation

knn_k_fold_cv <- function(data, k_CV = 5) {
  # Create the K-fold
  folds <- sample(1:k_CV, nrow(data), replace = TRUE)
  all_results <- list()

  # Perform K-fold cross-validation
  for(i in 1:k_CV) {
    # Split data into training and testing sets based on the fold
    train_data <- data[folds != i, ]
    test_data <- data[folds == i, ]
    
    # Run the classification pipeline
    results <- classification_pipeline(train_data, test_data)
    
    # Store results for this fold
    all_results[[i]] <- list(
      binary_false_positive_rate = results$binary_false_positive_rate,
      binary_false_negative_rate = results$binary_false_negative_rate,
      classification_match_rate = results$classification_match_rate,
      classification_fail_rate = results$classification_fail_rate
    )
  }
  
  # Store means
  binary_false_positive_rate_mean <- mean(sapply(all_results, function(x) x$binary_false_positive_rate))
  binary_false_negative_rate_mean <- mean(sapply(all_results, function(x) x$binary_false_negative_rate))
  classification_match_rate_mean <- mean(sapply(all_results, function(x) x$classification_match_rate))
  classification_fail_rate_mean <- mean(sapply(all_results, function(x) x$classification_fail_rate))

  avg_scores = list(binary_false_positive_rate_mean = binary_false_positive_rate_mean,
                     binary_false_negative_rate_mean = binary_false_negative_rate_mean,
                     classification_match_rate_mean = classification_match_rate_mean,
                     classification_fail_rate_mean = classification_fail_rate_mean)
  
  # Print all average metrics
  print("Average scores across all folds:")
  print(avg_scores)
  return(c(avg_scores=avg_scores, scores_by_fold=all_results))
}

# Example of using the function
#results <- k_fold_cv(data_matriz, k_CV = 4)
```


## Run KNN

### Run KNN on PCA Data


```{r, echo=FALSE, warning=FALSE, message=FALSE}

# Run PCA
pca_result <- PCA.fun(data)

# Get results and get PC's that explain 95% Variance
eig_vecs <- pca_result$"Eigen Vector"
cumulative_variance <- cumsum(pca_result$D)
num_components <- which(cumulative_variance >= 0.95)[1]

# Project data onto selected PCs
data_reduced <- as.matrix(data %>% dplyr::select(-Label)) %*% eig_vecs[, 1:num_components]

```


```{r}
# data test/train split
labels <- data$Label  
train_idx <- sample(1:nrow(data_reduced), 0.8 * nrow(data_reduced)) 
train_data_pca <- cbind(data_reduced[train_idx, ],labels[train_idx])
test_data_pca <- cbind(data_reduced[-train_idx, ],labels[-train_idx])

data_reduced_labeled = cbind(data_reduced,labels)
```


```{r}
results <- classification_pipeline(train_data,test_data)
#print(results$classification)
#print(results$classification_binary)
```

```{r}
# score
scores = score(results$classification,results$classification_binary)
scores
```
```{r}
CV_pca_results = knn_k_fold_cv(data_reduced_labeled, k_CV = 5)
```



## Run KNN on Raw Data


```{r}
CV_pca_results = knn_k_fold_cv(data_matriz, k_CV = 5)
```

```{r}
# Split the data
split_data <- train_test_split(data = data_matriz, train_ratio = 0.8, seed = NULL)
train_data <- split_data$train
test_data <- split_data$test

results <- classification_pipeline(train_data,test_data)
print(results$classification)
print(results$classification_binary)

print(results$type2_error)
```


























```{r}


rates = score(df_classification)
tpr = rates[1]
fnr = rates[2]
```















### Var2: Population Based
```{r}
### KNN Population Algo
knn_pop <- function(train_data, train_labels, test_data, k, threshold) {
  predictions <- c()
  
  ###
  for (i in 1:nrow(test_data)) {
    distances <- as.matrix(dist(rbind(test_data[i, ], train_data)))[1, -1]
    avg_distance <- mean(distances[k_neighbors])

    # Check if Population Threshold is passed
    if (avg_distance > threshold) {
      predicted_label <- "0"
    } else {
      # Run KNN for all groups
      # TODO: Optimize K
      k_neighbors <- order(distances)[1:k]
      neighbor_labels <- train_labels[k_neighbors] # get labels of neighbors
      predicted_label <- names(sort(table(neighbor_labels), decreasing = TRUE))[1] # make frequency table and pick most frequent 
      predicted_label <- names(sort(table(neighbor_labels), decreasing = TRUE))[1]
    }
    
    predictions <- c(predictions, predicted_label)
  }
  
  return(predictions)
}

```

### Run best KNN
```{r}

```

### Compare Results

TODO: Discuss Results

```{r}
# TODO: Display Results
```


# LDS/FDS

```{r}
# TODO: Linear Discriminant Analysis/Fisher discriminant Analysis
```













