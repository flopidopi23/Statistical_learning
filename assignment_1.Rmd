---
title: "Eigenfaces"
output: pdf_document
date: "2025-02-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(OpenImageR)
library(EBImage)
library(grid)
library(ggplot2)
library(caret)
library(pROC)

```

# Notes and Teach inputs

Assignment 1
raw_data -> KNN
raw_data -> PCA -> KNN


Steps:
1. Intra Class distances (same person) -> Visualize resulting histograms
2. Between Class distances (different persons) 
3. Set Threshold between Distance BetweeenClasses and Distance InterClass
4. Use difference Distances
5. Use PCA 

Assignment 2
1. FDA
raw_data -> PCA -> DiscrAnal(Fisher Faces) -> KNN


TODOS: 
- Apply and compare PCA : Flo 
- Read Image Name into read_data output: Flo
- CV Classification Hyperparam tuning - Raw Data : Simon
- CV Classification Hyperparam tuning - PCA Data : Simon
- Classification Test with Images that are not in Database: Simon

- Apply and interprete FDA (PCA -> FDA): Flo
- PCA -> FDA -> KNN (Hyperparm): Simon

- Prep Code for prof test data: Simon

- Introduction, Conclusion, make pretty : Simon


Notes from Class

Hyperparm tuning

KNN (Dist, k, treshhold)
PCA - KNN (Dist, k,treshhold, expl var)
PCA - FDA - KNN (Dist, k, treshhold, expl var, number of fda's)

to avoid computational cost. Take optimal from previous and only optimize new parameter
JUSTIFY! 



# Data Import and Preprocessing 

```{r}
# load selfmade functions
source("PCA_function.R")
source("read_data_function.R")
```


# Data Import and Preprocessing 


# Distance Histograms
```{r}
### Distance Histograms
distance_distributions <- function(data, distance_metric="euclidean"){
  
  ids <- data[, ncol(data)]
  
  # set up distance matrix/data frame
  dist_matrix <- as.matrix(dist(data[, -ncol(data)], method = distance_metric))
  dist_df <- data.frame(
    row1 = rep(1:nrow(data), each = nrow(data)),
    row2 = rep(1:nrow(data), times = nrow(data)),
    distance = as.vector(dist_matrix)
  )
  dist_df <- dist_df[dist_df$row1 != dist_df$row2, ] # remove self distances
  
  dist_df$id1 <- ids[dist_df$row1] # Assign IDs
  dist_df$id2 <- ids[dist_df$row2]
  
  dist_df_same_id <- dist_df[dist_df$id1 == dist_df$id2, ] # In group distances
  dist_df_same_id <- dist_df_same_id[order(dist_df_same_id$id1), ]
  
  
  dist_df_diff_id <- dist_df[dist_df$id1 != dist_df$id2, ] # Outside group distances
  
  dist_df_diff_id$id1 = 0
  dist_df_diff_id$id2 = 0
  
  dist_df_split <- rbind(dist_df_same_id,dist_df_diff_id)
  
  
  # Plot histogram
  plot = ggplot(dist_df_split, aes(x = distance, fill = factor(id1))) +
    geom_density(alpha = 0.6, position = "identity") +
    labs(title = "Histogram of Pairwise Distances by ID",
         x = "Distance", y = "Frequency", fill = "ID Group") +
    theme_minimal()
  
  return(plot)
    
}

```


# PCA


TODO: Make explained Variance plot

```{r}
### Verify with library
#out = princomp(X)

#pc1 = matrix(out$scores[,1],nrow=nrow(red), ncol=ncol(green))
#imageShow(pc1)

#pc2 = matrix(out$scores[,2],nrow=nrow(red), ncol=ncol(green))
#imageShow(pc2)

#pc3 = matrix(out$scores[,3],nrow=nrow(red), ncol=ncol(green))
#imageShow(pc3)
```








## Functions

### Test/Train Split

```{r}
# TODO: Test/Train Split
train_test_split <- function(data, train_ratio = 0.8, seed = NULL) {
  if (!is.null(seed)) {
    set.seed(seed)
  }
  # Split
  n = nrow(data)
  train_indices <- sample(1:n, size = floor(train_ratio * n))
  train_data <- data[train_indices, , drop = FALSE]  # Train set
  test_data <- data[-train_indices, , drop = FALSE]  # Test set

  return(list(train = train_data, test = test_data))
}
```

### KNN 

```{r}
### Distances

# Euclidean distance
euclidean_distance <- function(x, y) {
  sqrt(sum((x - y)^2))
}

# Manhattan distance
manhattan_distance <- function(x, y) {
  sum(abs(x - y))
}

```

```{r}
### Threshhold functions

## Criterion Functions
# precentile threshold function
estimate_threshold_via_percentile <- function(data, percentile = 0.90) {
  dist_1 <- as.matrix(dist(data))
  threshold <- quantile(dist_1[upper.tri(dist_1)], percentile)
  return(threshold)
}

## generate Treshhold functions
estimate_thresholds <- function(train_data, estimate_func, percentile) {
  thresholds <- numeric()
  train_data_ids_unique <- unique(train_data[, ncol(train_data)])
  
  for (id in train_data_ids_unique) {
    train_data_grouped <- train_data[train_data[, ncol(train_data)] == id, , drop = FALSE]
    if (nrow(train_data_grouped) > 1) {
      thresholds[id] <- estimate_func(train_data_grouped,percentile = percentile)
    }
  }
  
  return(thresholds)
}
```


```{r}
### KNN Classifier
knn_classifier = function(test_data, train_data, thresholds, dist_metric = "euclidean", k, knn_bool = T){
  # init results array
  predicted_ids = c() 
  
  # for all rows (images) run
  for (i in 1:nrow(test_data)) {
    
    ### Prepare Distances
    # For train data, get true ID and drop id col
    train_data_ids <- train_data[, ncol(train_data)]  # Extract ID column
    train_data_no_last_col <- train_data[, -ncol(train_data), drop = FALSE]
    
    # for test data, get row
    test_row = test_data[i,]
    
    # Compute distances between the test row and each row in the training data and assign IDs
    distances <- apply(train_data_no_last_col, 1, function(train_row) {
      test_row <- as.numeric(test_row)
      train_row <- as.numeric(train_row)
      
      if (dist_metric == "euclidean") {
        return(euclidean_distance(test_row, train_row))
      } else if (dist_metric == "manhattan") {
        return(manhattan_distance(test_row, train_row))
      }
    })
    names(distances) <- train_data_ids
  
    ### Classification
    predicted_label <- "0"
    # fot all groups IDs, check if threshhold is passed for at least one
    for (train_ids in names(thresholds)) { 
      # perform knn as soon as distance threshhod is passed for one
      if (any(distances[names(distances) == train_ids] < thresholds[train_ids])) { 
        if (knn_bool){
          k_neighbors <- order(distances)[1:min(k, length(distances))] # with k >= training points
          neighbor_labels <- train_data_ids[k_neighbors]  # Get labels of k-nearest neighbors
          
          # Get most frequent label
          predicted_label <- names(sort(table(neighbor_labels), decreasing = TRUE))[1]
          
          # store result
          predicted_ids = c(predicted_ids, predicted_label)
          break  # Stop once a group match is found
          
        } else{ # in case no KNN and only checking if in data or not 
          predicted_label = 1
          predicted_ids = c(predicted_ids, predicted_label)
          break
        }
      }
    }
    # if not passing any threshholds, assign group 0 (no group)
    if (predicted_label == "0") {
        predicted_ids = c(predicted_ids, predicted_label)
    }
  }
  
  # return predictions
  return(predicted_ids)
}
```


```{r}
### Scoring
score <- function(df_id,df_binary) {
  
  ### ID Classification Scoring
  
  # match & fail rate 
  match_rate <- mean(df_id$true == df_id$predicted)
  fail_rate <- 1 - match_rate
  
  ### Binary Classification Scoring
  # factorize
  df_binary$true = factor(df_binary$true, levels = c(0, 1))
  df_binary$predicted = factor(df_binary$predicted, levels = c(0, 1))

  # confusion with case of only values either 1 or 0
  unique_vals <- unique(c(df_binary$true, df_binary$predicted))
  if (all(unique_vals == 1) | all(unique_vals == 0)){
    return(list(binary_false_positive_rate = 0, 
              binary_false_negative_rate = 0,
              classification_match_rate = match_rate,
              classification_fail_rate = fail_rate))
  } else {
    conf_matrix <- table(df_binary$true, df_binary$predicted)
  }
  
  # Extract values
  TP <- conf_matrix[2, 2]  
  TN <- conf_matrix[1, 1]  
  FP <- conf_matrix[1, 2]  
  FN <- conf_matrix[2, 1]
  
  # Compute Metrics
  type1_error <- ifelse((FP + TN) == 0, 0, FP / (FP + TN))
  type2_error <- ifelse((FN + TP) == 0, 0, FN / (FN + TP))
  
  
  return(list(binary_false_positive_rate = type1_error, 
              binary_false_negative_rate = type2_error,
              classification_match_rate = match_rate,
              classification_fail_rate = fail_rate))
}
```


```{r}
### Pipeline
classification_pipeline <- function(train_data,
                                    test_data,
                                    estimate_func,
                                    percentile,
                                    knn_k,
                                    dist_metric,
                                    knn_bool = T) {
  
  # Replace IDs in test_data that are not in train_data with "0"
  train_ids <- train_data[, ncol(train_data)]
  test_ids <- test_data[, ncol(test_data)]
  test_data[, ncol(test_data)][!test_ids %in% train_ids] <- 0
  
  # Estimate thresholds
  thresholds <- estimate_thresholds(train_data, estimate_func = estimate_func, percentile = percentile)
  
  # Prepare test data
  true_test_ids <- c(test_data[, ncol(test_data), drop = FALSE])
  test_data_input <- test_data[, -ncol(test_data), drop = FALSE]
  
  # Run KNN classifier
  predicted_ids <- knn_classifier(test_data_input, train_data, thresholds = thresholds, dist_metric = dist_metric,k = knn_k,knn_bool = knn_bool)
  
  
  # Merge results
  df_classification <- data.frame(
    true = as.numeric(true_test_ids),
    predicted = as.numeric(predicted_ids)
  )
  
  # Derive binary classification [0,1] (if in data set)
  df_classification_binary <- as.data.frame(lapply(df_classification, function(x) ifelse(x == 0, 0, 1)))
  
  # Calculate score
  scores <- score(df_id = df_classification, df_binary = df_classification_binary)
  
  # Return results
  return(list(
    classification = df_classification,
    classification_binary = df_classification_binary,
    binary_false_positive_rate = scores$binary_false_positive_rate, 
    binary_false_negative_rate = scores$binary_false_negative_rate,
    classification_match_rate = scores$classification_match_rate,
    classification_fail_rate = scores$classification_fail_rate
  ))
}

```




```{r}
### K fold Cross Validation
knn_k_fold_cv <- function(data,
                          folds,
                          k_CV,
                          estimate_func, 
                          percentile, 
                          knn_k, 
                          dist_metric,
                          print_bool = F) {

  all_results <- list()
  # Perform K-fold cross-validation
  for(i in 1:k_CV) {
    
    # Split data into training and testing sets based on the fold
    train_data <- data[folds != i, ]
    test_data <- data[folds == i, ]
    
    # Run the classification pipeline
    results <- classification_pipeline(train_data, 
                                       test_data,
                                       estimate_func = estimate_func,
                                       percentile = percentile,
                                       knn_k = knn_k ,
                                       dist_metric = dist_metric)
    
    # Store results for this fold
    all_results[[i]] <- list(
      binary_false_positive_rate = results$binary_false_positive_rate,
      binary_false_negative_rate = results$binary_false_negative_rate,
      classification_match_rate = results$classification_match_rate,
      classification_fail_rate = results$classification_fail_rate
    )
  }
  
  # Store means
  binary_false_positive_rate_mean <- mean(sapply(all_results, function(x) x$binary_false_positive_rate))
  binary_false_negative_rate_mean <- mean(sapply(all_results, function(x) x$binary_false_negative_rate))
  classification_match_rate_mean <- mean(sapply(all_results, function(x) x$classification_match_rate))
  classification_fail_rate_mean <- mean(sapply(all_results, function(x) x$classification_fail_rate))

  avg_scores = list(binary_false_positive_rate_mean = binary_false_positive_rate_mean,
                     binary_false_negative_rate_mean = binary_false_negative_rate_mean,
                     classification_match_rate_mean = classification_match_rate_mean,
                     classification_fail_rate_mean = classification_fail_rate_mean)
  
  # Print all average metrics
  if (print_bool){
    print("Average scores across all folds:")
    print(avg_scores)
  }

  return(avg_scores)#, scores_by_fold=all_results))
}

# Example of using the function
#results <- k_fold_cv(data_matriz, k_CV = 4)
```


```{r}
### Threshhold optimization
treshhold_optimizer <- function(data,
                          estimate_func, 
                          percentile_values, 
                          knn_k = 1, # irrelavent 
                          dist_metrics,
                          print_bool = F) {

  results_list <- list()
  # Perform K-fold cross-validation
  
  ids <- data[, ncol(data)]  
  
  unique_ids <- unique(ids)  
  
  i = 1 
  for(id in unique_ids) {
    for (percentile in percentile_values) {
      for (dist_metric in dist_metrics) {
        test_data <- data[ids == id, , drop = FALSE]
        train_data <- data[ids != id, , drop = FALSE]
        
        # Run the classification pipeline
        results <- classification_pipeline(train_data, 
                                           test_data,
                                           estimate_func = estimate_func,
                                           percentile = percentile,
                                           knn_k = knn_k ,
                                           dist_metric = dist_metric,
                                           knn_bool = F)
        
          # Store results for this fold
          results_list[[i]] <- list(
            knn_k = knn_k,
            percentile = percentile,
            dist_metric = dist_metric,
            binary_false_positive_rate = results$binary_false_positive_rate,
            binary_false_negative_rate = results$binary_false_negative_rate,
            classification_match_rate = results$classification_match_rate,
            classification_fail_rate = results$classification_fail_rate
          )
          i = i+1
      }
    }
  }
  
  
  # Convert results to data frame
  results_df <- do.call(rbind, lapply(results_list, as.data.frame))
  
  # Find the best parameters for threshold and for knn
  best_params_knn <- results_df[which.max(results_df$classification_match_rate_mean), ]
  best_params_binary <- results_df[which.min(results_df$binary_false_negative_rate_mean), ]
  
  print("#################### Best Hyperparameters ####################")
  print(paste("Distance Metric: ", best_params_knn$dist_metric))
  print(paste("Percentile for Quantile Threshold: ", best_params_binary$percentile))
  print(paste("KNN K nearest neighbors: ", best_params_knn$knn_k))

  return(results_df)#, scores_by_fold=all_results))
}

# Example of using the function
#results <- k_fold_cv(data_matriz, k_CV = 4)
```


```{r}
result = knn_k_fold_binary(data_matriz,
                          estimate_func = estimate_threshold_via_percentile , 
                          percentile_values = c(0.9,0.95,0.99), 
                          knn_k = 1, # irrelavent 
                          dist_metrics = c("euclidean", "manhattan"),
                          print_bool = F)
result 
```


```{r}
CV_grid_search <- function(data, knn_k_values, percentile_values, dist_metrics, k_CV = 5) {
  # Store results
  results_list <- list()
  
  folds <- sample(1:k_CV, nrow(data), replace = TRUE)

  # Perform grid search
  for (knn_k in knn_k_values) {
    for (percentile in percentile_values) {
      for (dist_metric in dist_metrics) {
        
        # Run cross-validation
        avg_scores <- knn_k_fold_cv(data,
                                    folds,
                                    k_CV = k_CV,
                                    estimate_func = estimate_threshold_via_percentile,
                                    percentile = percentile,
                                    knn_k = knn_k,
                                    dist_metric = dist_metric)
        
        # Store results
        results_list[[paste(knn_k, percentile, dist_metric, sep="_")]] <- list(
          knn_k = knn_k,
          percentile = percentile,
          dist_metric = dist_metric,
          binary_false_positive_rate_mean = avg_scores$binary_false_positive_rate_mean,
          binary_false_negative_rate_mean = avg_scores$binary_false_negative_rate_mean,
          classification_match_rate_mean = avg_scores$classification_match_rate_mean,
          classification_fail_rate_mean = avg_scores$classification_fail_rate_mean
        )
      }
    }
  }
  
  # Convert results to data frame
  results_df <- do.call(rbind, lapply(results_list, as.data.frame))
  
  # Find the best parameters for threshold and for knn
  best_params_knn <- results_df[which.max(results_df$classification_match_rate_mean), ]
  best_params_binary <- results_df[which.min(results_df$binary_false_negative_rate_mean), ]
  
  print("#################### Best Hyperparameters ####################")
  print(paste("Distance Metric: ", best_params_knn$dist_metric))
  print(paste("Percentile for Quantile Threshold: ", best_params_binary$percentile))
  print(paste("KNN K nearest neighbors: ", best_params_knn$knn_k))

  return(list(results_df = results_df, 
              best_params_knn = best_params_knn, 
              best_params_binary = best_params_binary))
}
```





```{r}
### Predict for new data
knn_classifier_full <- function(path_train, 
                           path_test,                               
                           dist_metric,
                           knn_k,
                           result, 
                           estimate_func = estimate_threshold_via_percentile,
                           percentile = percentile) {

  
  data_train = read_all_images(path_train)
  data_test = read_all_images(path_test)
  
  image_names = data_test$ID

  data_train_reduced = data_train %>% dplyr::select(-ID)
  data_test_reduced = data_test %>% dplyr::select(-ID)
  
  data_train_asmatrix = as.matrix(data_train_reduced)
  data_test_asmatrix = as.matrix(data_test_reduced)
  
  #data_test_asmatrix[, ncol(data_test_asmatrix)][!labels_test_init %in% labels_train] <- 0
  #labels_test = data_test_asmatrix[,ncol(data_test_asmatrix)]
  
  
  classification_result = classification_pipeline(train_data = data_train_asmatrix,
                                    test_data = data_test_asmatrix,
                                    estimate_func = estimate_func,
                                    percentile = percentile,
                                    knn_k = knn_k,
                                    dist_metric = dist_metric)
  
  classification_result$classification = cbind(image_names,classification_result$classification)
  classification_result$classification_binary = cbind(image_names,classification_result$classification_binary)

  
  # Return result
  return(classification_result)
}
```


# Run Codes

## Load Data
```{r}
folder_path = "Training"
data = read_all_images(folder_path)

labels <- data$Label  
image_names = data$ID

data = data %>% dplyr::select(-ID)

data_matriz = as.matrix(data)
```


## Distance Histograms

```{r}
# Run
for (distance_metric in c("euclidean", "manhattan")) {
  hist = distance_distributions(data_matriz, distance_metric = distance_metric)
  print(hist)
}

```

## Run KNN

## Find Optimal Treshhold Params 

```{r}
result = knn_k_fold_binary(data_matriz,
                          estimate_func = estimate_threshold_via_percentile , 
                          percentile_values = c(0.9,0.95,0.99), 
                          knn_k = 1, # irrelavent 
                          dist_metrics = c("euclidean", "manhattan"),
                          print_bool = F)
result 
```


### Run KNN on Raw Data

```{r}
### Run grid Search to optimize Hyperparameters
CV_grid_search_result <- CV_grid_search(data_reduced_labeled,
                                        k_CV = 5,
                                        knn_k_values = c(3, 4, 5, 6, 7), 
                                        percentile_values = c(0.8, 0.9, 0.95, 0.99), 
                                        dist_metrics = c("euclidean", "manhattan"))

#print(CV_grid_search_result$results_df)
print(CV_grid_search_result$best_params_knn)
#print(CV_grid_search_result$best_params_binary)

```


### Run KNN on PCA Data


```{r}
### Run PCA & prep data
pca_result <- PCA.fun(data)

# Get results and get PC's that explain 95% Variance
eig_vecs <- pca_result$"Eigen Vector"
cumulative_variance <- cumsum(pca_result$D)
num_components <- which(cumulative_variance >= 0.95)[1]

# Project data onto selected PCs
data_reduced <- as.matrix(data %>% dplyr::select(-Label)) %*% eig_vecs[, 1:num_components]

data_reduced_labeled = cbind(data_reduced, labels)
```


```{r}
### Run grid Search to optimize Hyperparameters
CV_grid_search_result <- CV_grid_search(data_reduced_labeled,
                                        k_CV = 5,
                                        knn_k_values = c(3, 4, 5, 6, 7), 
                                        percentile_values = c(0.8, 0.9, 0.95, 0.99), 
                                        dist_metrics = c("euclidean", "manhattan"))

#print(CV_grid_search_result$results_df)
print(CV_grid_search_result$best_params_knn)
print(CV_grid_search_result$best_params_binary)

```


```{r}

```



### Compare Results

TODO: Discuss Results

```{r}
# TODO: Display Results
```




## Run KNN for new Data on best Combination

```{r}
### Predict for new Data
results = knn_classifier_full( path_train = "Training_alpha", 
                               path_test = "Test_alpha",
                               dist_metric = 'euclidean',
                               knn_k = 6,
                               estimate_func = estimate_threshold_via_percentile,
                               percentile = 0.8)

results
```





















### Compare Results














